{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import faiss  # The FAISS library\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NVIDIA_API_KEY = os.getenv(\"NVIDIA_API_KEY\")\n",
    "\n",
    "EMBED_MODEL = \"nvidia/llama-3.2-nv-embedqa-1b-v2\"\n",
    "RERANK_MODEL = \"nvidia/llama-3.2-nv-rerankqa-1b-v2\"\n",
    "LLM_MODEL    = \"meta/llama-3.1-70b-instruct\"\n",
    "\n",
    "embedding_client = OpenAI(\n",
    "    api_key=NVIDIA_API_KEY,\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\"\n",
    ")\n",
    "\n",
    "llm_client = OpenAI(\n",
    "    api_key=NVIDIA_API_KEY,\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=500, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = min(start + chunk_size, len(words))\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start += (chunk_size - overlap)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def embed_text(text: str) -> np.ndarray:\n",
    "    response = embedding_client.embeddings.create(\n",
    "        input=[text],\n",
    "        model=EMBED_MODEL,\n",
    "        encoding_format=\"float\",\n",
    "        extra_body={\"input_type\": \"passage\", \"truncate\": \"NONE\"}\n",
    "    )\n",
    "    return np.array(response.data[0].embedding, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_map = {}\n",
    "all_embeddings = []\n",
    "all_ids = [] \n",
    "\n",
    "faiss_index = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_document_to_index(doc_text: str, start_id=0):\n",
    "    \"\"\"\n",
    "    Chunk the document, embed each chunk, and add to a FAISS index.\n",
    "    Returns the last used ID so you can continue numbering for the next doc.\n",
    "    \"\"\"\n",
    "    global faiss_index, doc_map\n",
    "    \n",
    "    chunks = chunk_text(doc_text)\n",
    "    \n",
    "    chunk_vectors = []\n",
    "    chunk_ids = []\n",
    "    current_id = start_id\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        vec = embed_text(chunk)\n",
    "        chunk_vectors.append(vec)\n",
    "        chunk_ids.append(current_id)\n",
    "        \n",
    "        doc_map[current_id] = chunk\n",
    "        current_id += 1\n",
    "    \n",
    "    chunk_vectors_np = np.vstack(chunk_vectors)\n",
    "    chunk_ids_np = np.array(chunk_ids, dtype=np.int64)\n",
    "    \n",
    "    if faiss_index is None:\n",
    "        embedding_dim = chunk_vectors_np.shape[1]\n",
    "        index_flat = faiss.IndexFlatL2(embedding_dim)\n",
    "        # index_flat = faiss.IndexFlatIP(embedding_dim)\n",
    "        index_with_ids = faiss.IndexIDMap(index_flat)\n",
    "        faiss_index = index_with_ids\n",
    "    \n",
    "    faiss_index.add_with_ids(chunk_vectors_np, chunk_ids_np)\n",
    "    \n",
    "    return current_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index size: 1\n"
     ]
    }
   ],
   "source": [
    "sample_doc = \"\"\"\n",
    "NVIDIA H100 GPUs deliver unprecedented acceleration, \n",
    "featuring 3 TB/s of memory bandwidth. In combination \n",
    "with Grace CPU, it can achieve 900GB/s of chip-to-chip \n",
    "bandwidth, enabling blazing-fast HPC and AI workloads.\n",
    "\"\"\"\n",
    "\n",
    "last_id = add_document_to_index(sample_doc, start_id=0)\n",
    "print(\"Index size:\", faiss_index.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_top_k_faiss(query: str, k=3):\n",
    "    query_vec = embed_text(query)\n",
    "    query_vec_2d = np.expand_dims(query_vec, axis=0)\n",
    "    \n",
    "    distances, ids = faiss_index.search(query_vec_2d, k)\n",
    "    results = []\n",
    "\n",
    "    for dist, doc_id in zip(distances[0], ids[0]):\n",
    "        if doc_id == -1:\n",
    "            continue\n",
    "        passage_text = doc_map[doc_id]\n",
    "        results.append((passage_text, dist))\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "RERANK_URL = \"https://ai.api.nvidia.com/v1/retrieval/nvidia/llama-3_2-nv-rerankqa-1b-v2/reranking\"\n",
    "\n",
    "def rerank_passages(query: str, passages: list):\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {NVIDIA_API_KEY}\",\n",
    "        \"Accept\": \"application/json\",\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": RERANK_MODEL,\n",
    "        \"query\": {\"text\": query},\n",
    "        \"passages\": [{\"text\": p} for p in passages]\n",
    "    }\n",
    "    url = \"https://ai.api.nvidia.com/v1/retrieval/nvidia/llama-3_2-nv-rerankqa-1b-v2/reranking\"\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    response.raise_for_status()\n",
    "    response_data = response.json()\n",
    "\n",
    "    rankings = response_data[\"rankings\"]\n",
    "\n",
    "    results = []\n",
    "    for r in rankings:\n",
    "        idx = r[\"index\"]\n",
    "        logit = r[\"logit\"]\n",
    "        passage_text = passages[idx]\n",
    "        results.append({\"text\": passage_text, \"score\": logit})\n",
    "    \n",
    "    sorted_results = sorted(results, key=lambda x: x[\"score\"], reverse=True)\n",
    "    return sorted_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_answer(query: str, passages: list, max_tokens=512):\n",
    "    \"\"\"\n",
    "    Uses meta/llama-3.1-70b-instruct to generate a final answer\n",
    "    given the query and a list of top passages.\n",
    "    \"\"\"\n",
    "\n",
    "    context_text = \"\\n\\n\".join(passages)\n",
    "    \n",
    "    system_prompt = (\n",
    "        \"You are an AI assistant specialized in GPU technology. \"\n",
    "        \"You have access to the following text snippets:\\n\\n\"\n",
    "        f\"{context_text}\\n\\n\"\n",
    "        \"Use these snippets to answer the user query accurately. \"\n",
    "        \"If you are unsure, just say you don't know.\"\n",
    "    )\n",
    "    \n",
    "    user_prompt = f\"User Query: {query}\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    \n",
    "    completion = llm_client.chat.completions.create(\n",
    "        model=LLM_MODEL,\n",
    "        messages=messages,\n",
    "        temperature=0.2,\n",
    "        top_p=0.7,\n",
    "        max_tokens=max_tokens,\n",
    "        stream=False\n",
    "    )\n",
    "    \n",
    "    return completion.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query: str, k=5, final_context_count=3):\n",
    "    faiss_results = retrieve_top_k_faiss(query, k=k)\n",
    "    passages = [r[0] for r in faiss_results]\n",
    "    reranked_passages = rerank_passages(query, passages)\n",
    "    final_passages = [p[\"text\"] for p in reranked_passages[:final_context_count]]\n",
    "    answer = generate_final_answer(query, final_passages)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rankings': [{'index': 0, 'logit': 22.75}], 'usage': {'prompt_tokens': 64, 'total_tokens': 64}}\n",
      "Final Answer:\n",
      " The memory bandwidth of the NVIDIA H100 GPU is 3 TB/s.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"What is the memory bandwidth of the H100 GPU?\"\n",
    "final_answer = answer_query(user_question)\n",
    "print(\"Final Answer:\\n\", final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx-nokia-hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
